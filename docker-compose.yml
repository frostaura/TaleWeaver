version: "3.9"

services:
  # LLM service for story generation (CPU-optimized)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    environment:
      - OLLAMA_NUM_PARALLEL=1  # prevent CPU overload

  # TTS service (CPU-only)
  parler-tts:
    image: ghcr.io/huggingface/text-to-speech:latest
    container_name: parler-tts
    environment:
      - MODEL_ID=parler-tts/parler-tts-mini
      - DEVICE=cpu
    ports:
      - "5005:5005"
    restart: unless-stopped

  # TaleWeaver backend API
  backend:
    build:
      context: ./src/backend
      dockerfile: Dockerfile
    container_name: taleweaver-backend
    environment:
      - ASPNETCORE_ENVIRONMENT=Production
      - ASPNETCORE_URLS=http://+:5000
      - OllamaUrl=http://ollama:11434
      - ParlerTtsUrl=http://parler-tts:5005
    ports:
      - "5000:5000"
    depends_on:
      - ollama
      - parler-tts
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/story/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # TaleWeaver frontend
  frontend:
    build:
      context: ./src/frontend
      dockerfile: Dockerfile
    container_name: taleweaver-frontend
    ports:
      - "3000:80"
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  ollama_data: